# ğŸš€ Hand Gesture Interaction System with Extended Kalman Filter (EKF)
### Realâ€‘time Hand Tracking â€¢ Gesture Recognition â€¢ Air Drawing â€¢ Virtual Mouse â€¢ Volume Control

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8.1.78-success)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10.8-orange)
![NumPy](https://img.shields.io/badge/NumPy-1.24.3-informational)
![SciPy](https://img.shields.io/badge/SciPy-1.11.4-informational)

A modular **Computer Vision + HCI** project that turns hand movements into touchless control.
It tracks **3D hand landmarks** in real time, recognizes gestures, and stabilizes motion using an **Adaptive Extended Kalman Filter (EKF)** for smoother interaction.

---

## âœ¨ Features

- ğŸ–± **Virtual Mouse**: smooth cursor movement + click/drag + twoâ€‘finger scroll
- ğŸ¨ **Air Drawing**: draw in the air with an onâ€‘screen canvas + clear canvas shortcut
- ğŸ”Š **Volume Control**: pinch distance â†’ volume mapping *(Linux `amixer` integration)*
- âœ‹ **Gesture Recognition**: static + dynamic gestures with **temporal smoothing**
- ğŸ“‰ **Adaptive EKF Stabilization**: reduces jitter and improves reliability
- ğŸ§© **Modeâ€‘based architecture**: easy to add new interaction modes

---

## ğŸ§  How it works

**Camera â†’ HandTracker â†’ ThreadedPipeline â†’ GestureRecognizer â†’ Mode â†’ Controller â†’ OS**

- `hand_tracker.py` extracts MediaPipeâ€™s 21 hand landmarks and smooths motion using `AdaptiveExtendedKalmanFilter`.
- `gesture_recognizer.py` computes gesture confidence from landmark geometry and applies **temporal voting** for stability.
- `modes/` define interaction behavior (mouse/draw/volume/demo).
- `controllers.py` translates mode outputs to OS actions (mouse movement/clicks/scroll and volume control).

---

## ğŸ® Keyboard shortcuts (OpenCV window)

| Key | Action |
|---:|---|
| `m` | Mouse mode |
| `d` | Draw mode |
| `v` | Volume mode |
| `Space` | Demo mode *(no side effects)* |
| `c` | Clear canvas *(draw mode)* |
| `q` | Quit |

*(Handled in `core/orchestrator.py`.)*

---

## âœ‹ Supported gestures

Implemented in `gesture_recognizer.py`:

### Static
- `Point`
- `Thumbs_Up`
- `Peace`
- `OK`
- `Fist`
- `Open_Palm`

### Pinch
- `Pinch_TI` *(thumbâ€“index)*
- `Pinch_TM` *(thumbâ€“middle)*

### Dynamic
- `TwoFinger_Scroll_Up`, `TwoFinger_Scroll_Down`
- `Swipe_Left`, `Swipe_Right`, `Swipe_Up`, `Swipe_Down`

> Modes apply context filtering (e.g., volume mode focuses on `Pinch_TI`).

---

## ğŸ“¦ Installation

### 1) Clone
```bash
git clone <YOUR_REPO_URL>
cd hand-tracking-3d
```

### 2) Create and activate a virtual environment
**Windows**
```bash
python -m venv .venv
.venv\Scripts\activate
```

**macOS / Linux**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3) Install dependencies
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Run
```bash
python main.py
```

A window will open. Use the keyboard shortcuts to switch modes.

---

## ğŸ”Š Volume control (important note)

Volume control is implemented in `controllers.py` via Linux command:

- `amixer set Master <N>%`

âœ… Works on Linux **when** `amixer` exists and the mixer control is named `Master`.  
âš ï¸ On Windows/macOS, the onâ€‘screen volume indicator still updates, but system volume may not change.

---

## ğŸ§ª Tests
```bash
python -m pytest -q
```

Tests are in `tests/test_gesture_recognizer.py`.

---

## ğŸ—‚ï¸ Project structure

```
hand-tracking-3d/
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ app.py                  # App wiring/lifecycle helpers
â”œâ”€â”€ gui.py                  # UI overlays and drawing helpers
â”œâ”€â”€ hand_tracker.py         # MediaPipe landmarks + EKF smoothing + pinch metrics
â”œâ”€â”€ gesture_recognizer.py   # Gesture classification + temporal smoothing
â”œâ”€â”€ controllers.py          # OS actions (mouse/scroll/volume)
â”œâ”€â”€ constants.py            # Tunable thresholds and constants
â”œâ”€â”€ utils.py                # Shared helpers
â”œâ”€â”€ camera_diag.py          # Camera diagnostics tool
â”œâ”€â”€ hand_model_demo.py      # Hand model / visualization demo
â”‚
â”œâ”€â”€ core/
â”‚   â””â”€â”€ orchestrator.py     # Main runtime loop + mode switching + rendering
â”‚
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ threaded_pipeline.py# Threaded capture/processing for stable latency
â”‚
â”œâ”€â”€ modes/
â”‚   â”œâ”€â”€ base.py             # Mode interface
â”‚   â”œâ”€â”€ mouse_mode.py       # Virtual mouse logic
â”‚   â”œâ”€â”€ draw_mode.py        # Air drawing logic
â”‚   â”œâ”€â”€ volume_mode.py      # Volume mapping logic
â”‚   â””â”€â”€ demo_mode.py        # Read-only demo mode
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_gesture_recognizer.py
```

---

## ğŸ§¹ GitHub: what NOT to commit

Add this `.gitignore` (recommended):

```gitignore
__pycache__/
*.pyc
.pytest_cache/
.venv/
.vscode/
.DS_Store
Thumbs.db
```

---

## ğŸ“„ License

Add a `LICENSE` file (MIT is a common choice) and update this section accordingly.

